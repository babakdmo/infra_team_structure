# **Data Platform Team**

## ðŸ§¾ Team Summary

| Area | Responsibilities |
|------|------------------|
| **Services Owned** | HDFS, YARN, Spark, Airflow, Kafka, ZooKeeper (on Kubernetes) |
| **Deployment & Upgrades** | Helm charts, Operators, rolling upgrades for zero downtime |
| **Configuration** | JVM tuning, Spark executor resources, Airflow concurrency |
| **Security** | FreeIPA/Kerberos integration, TLS certificates, NetworkPolicies |
| **Monitoring** | Prometheus, Grafana, Loki, custom alerts |
| **Backup & DR** | Snapshots, failover scripts, disaster recovery testing |

---

## ðŸ”§ Owns:
- HDFS
- YARN
- Airflow
- Spark
- ZooKeeper
- Kafka  
(All deployed and managed on Kubernetes)

---

## ðŸ§© Core Responsibilities

### 1. Service Deployment & Upgrades
- Package and deploy services using Helm charts or Operators:
  - HDFS NameNode/DataNode
  - Spark Operator
  - Airflow with KubernetesExecutor
  - Kafka and ZooKeeper clusters
- Plan and execute **rolling upgrades** for Hadoop, Spark, and Airflow to ensure **zero downtime**.

### 2. Configuration & Tuning
- Tune HDFS replication, JVM GC settings for NameNode.
- Configure Spark executor memory and core settings.
- Tune Airflow parallelism and DAG concurrency.
- Manage YARN queue capacities and scheduler settings (FIFO vs FAIR).

### 3. Security & Access
- Integrate FreeIPA/Kerberos inside Kubernetes pods for HDFS/YARN auth.
- Issue and rotate TLS certs for internal services.
- Apply Kubernetes NetworkPolicies for inter-service access control.

### 4. Observability & Alerting
- Deploy Prometheus exporters for:
  - HDFS
  - YARN
  - Spark
- Set up Grafana dashboards and Loki/EFK for logging.
- Define alerts for:
  - Disk saturation
  - Failed jobs
  - Scheduler throughput drops

### 5. Backup, Recovery & Disaster Recovery (DR)
- Automate:
  - HDFS NameNode metadata snapshots (fsimage/JN edits)
  - Airflow DB dumps
- Script:
  - NameNode failover
  - ZooKeeper quorum recovery
- Quarterly test of restore and recovery processes.

---

## ðŸ“‹ Sample Subtasks

| Subtask | Framework / Tool |
|---------|-------------------|
| Install HDFS via Helm and configure DataNode volumes | Helm, Kubernetes CSI |
| Deploy SparkOperator and submit PySpark job with dynamic allocation | Spark Operator |
| Setup Airflow with PostgreSQL backend and Git-based DAG sync | Airflow Helm Chart, GitSync |
| Create PrometheusRule for HDFS under-replicated blocks and Slack alerts | Prometheus, Alertmanager |
| Schedule daily Airflow metadata backup to S3 (MinIO) | Kubernetes CronJob, MinIO CSI |
